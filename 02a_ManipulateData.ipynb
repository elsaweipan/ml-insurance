{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['uniqueID', 'Var01', 'Var03', 'Var07', 'Var10', 'Var11', 'Var13', 'Var14', 'Var19', 'Var20', 'Var21', 'Var23', 'Var25', 'Var26', 'Var27', 'Var28', 'Var29', 'Var30', 'Var31', 'Var32', 'Var33', 'Var34', 'Var35', 'Var42', 'pred_s', 'Var47', 'Var48', 'Var49', 'Var50', 'Var51', 'Var52', 'Var53', 'Var55', 'c01', 'c02', 'c03', 'c06', 'c07', 'c08', 'c09', 'c10', 'c11', 'c12', 'c13', 'c14', 'c15', 'c17', 'c18', 'c19', 'c20', 'c21', 'c22', 'c23', 'c24', 'c25', 'c27', 'c28', 'c29', 'c30', 'c31', 'c32', 'c33', 'c36', 'c37', 'Var54', 'Var56', 'Var02', 'Var04', 'Var05', 'Var06', 'Var08', 'Var09', 'Var12', 'Var15', 'Var16', 'Var18', 'Var22', 'Var24', 'c04', 'c05', 'c16', 'c26', 'c34', 'c35', 'c38', 'c39', 'c40', 'id', 'train']\n"
     ]
    }
   ],
   "source": [
    "## read the data\n",
    "with open('../final_project_data/insurance_clf_data.pickle','rb') as file:\n",
    "    df_all = pickle.load(file)\n",
    "    \n",
    "print(list(df_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop duplications\n",
    "df_all = df_all.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = ['Var47']\n",
    "\n",
    "labels = ['uniqueID','id', 'train']\n",
    "\n",
    "features = ['Var01', 'Var03', 'Var07', 'Var10', 'Var11', 'Var13',\n",
    "             'Var14', 'Var19', 'Var20', 'Var21', 'Var23', 'Var25', 'Var26',\n",
    "             'Var27', 'Var28', 'Var29', 'Var30', 'Var31', 'Var32', 'Var33', \n",
    "             'Var34', 'Var35', 'Var42', 'Var48', 'Var49', \n",
    "             'Var50', 'Var51', 'Var52', 'Var53', 'Var55', \n",
    "             'c01', 'c02', 'c03', 'c06', 'c07', 'c08', 'c09', 'c10', 'c11', 'c12', \n",
    "             'c13', 'c14', 'c15', 'c17', 'c18', 'c19', 'c20', 'c21', 'c22', 'c23', \n",
    "             'c24', 'c25', 'c27', 'c28', 'c29', 'c30', 'c31', 'c32', 'c33', 'c36', 'c37', \n",
    "             'Var54', 'Var56', 'Var02', 'Var04', 'Var05', 'Var06', 'Var08', 'Var09', \n",
    "             'Var12', 'Var15', 'Var16', 'Var18', 'Var22', 'Var24', \n",
    "             'c04', 'c05', 'c16', 'c26', 'c34', 'c35', 'c38', 'c39', 'c40']\n",
    "\n",
    "\n",
    "na_indicator_cols = ['Var10', 'Var26', 'Var28', 'c08','c11', 'c12', 'c15',\n",
    "                          'c18', 'c19', 'c27', 'Var12', 'c04', 'c05', 'c16', 'c26', \n",
    "                          'c34', 'c38', 'c39', 'c40',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create indicators for missing values\n",
    "df_na_indicators = df_all[na_indicator_cols].isnull().astype(int).add_suffix('_NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Impute missing values \n",
    "## [Source] https://stackoverflow.com/questions/25239958/impute-categorical-missing-values-in-scikit-learn\n",
    "\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "class DataFrameImputer(TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Impute missing values.\n",
    "\n",
    "        Columns of dtype object are imputed with the most frequent value \n",
    "        in column.\n",
    "\n",
    "        Columns of other types are imputed with median of column.\n",
    "\n",
    "        \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "\n",
    "        self.fill = pd.Series([X[c].value_counts().index[0]\n",
    "            if X[c].dtype == np.dtype('O') else X[c].median() for c in X],index=X.columns)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return X.fillna(self.fill)\n",
    "\n",
    "df_features_filled = DataFrameImputer().fit_transform(df_all[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "## One-hot Encoding\n",
    "df_features_filled_ohe = pd.get_dummies(df_features_filled)\n",
    "## Combine to one dataframe\n",
    "df_all_b = pd.concat([df_features_filled_ohe, df_na_indicators,\n",
    "                      df_all[['train','Var47','pred_s', 'uniqueID','id']]], axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_all_b.head().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the data after maniputation\n",
    "with open('../final_project_data/insurance_clf_data_new.pickle','wb') as file:\n",
    "    pickle.dump(df_all_b, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
