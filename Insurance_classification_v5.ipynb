{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q. Any idea on managing these 200+ features after ohe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UewBJeoOQJPS"
   },
   "outputs": [],
   "source": [
    "# ## for colab\n",
    "# # !pip install -U -q PyDrive\n",
    "\n",
    "# from pydrive.auth import GoogleAuth\n",
    "# from pydrive.drive import GoogleDrive\n",
    "# from google.colab import auth\n",
    "# from oauth2client.client import GoogleCredentials\n",
    "\n",
    "# auth.authenticate_user()\n",
    "# gauth = GoogleAuth()\n",
    "# gauth.credentials = GoogleCredentials.get_application_default()\n",
    "# drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7fm4rNqJQJPV"
   },
   "outputs": [],
   "source": [
    "# ## for colab\n",
    "# # Load the Drive helper and mount\n",
    "# from google.colab import drive\n",
    "\n",
    "# # This will prompt for authorization.\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_7JDkhojQJPe"
   },
   "outputs": [],
   "source": [
    "## common imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VdOROy68ePGT"
   },
   "outputs": [],
   "source": [
    "## to plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "## to make this notebook's output stable across runs\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wbdc3bSlQJPX"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ie-NfyEAQJPh"
   },
   "outputs": [],
   "source": [
    "## import sklearn packages\n",
    "# from sklearn.metrics import precision_score, recall_score\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "# from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "# from sklearn.metrics import precision_recall_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-1rcyJUyQJPd"
   },
   "source": [
    "# 01_ReadData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4CgvBUrYQJPi"
   },
   "outputs": [],
   "source": [
    "## for colab \n",
    "## read the data\n",
    "# df_all = pd.read_csv('/content/drive/My Drive/Final Project_Nisal David Elsa/insurance_classification_data.csv')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read in the raw data to a data frame and saves it to pickle\n",
    "# df_all = pd.read_csv('../final_project_data/insurance_classification_data.csv')\n",
    "# with open('../final_project_data/insurance_clf_data.pickle','wb') as file:\n",
    "#     pickle.dump(df_all, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read the data\n",
    "with open('../final_project_data/insurance_clf_data.pickle','rb') as file:\n",
    "    df_all = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gBgm9MgRQJPk",
    "outputId": "c97a6036-c97e-44ac-a678-dacab321a9d6"
   },
   "outputs": [],
   "source": [
    "df_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DXsVhX9-QJPq",
    "outputId": "8d65bbdb-01e7-4ac9-a11e-14fbd42c0f42",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1794135 entries, 0 to 1795671\n",
      "Data columns (total 89 columns):\n",
      "uniqueID    int64\n",
      "Var01       object\n",
      "Var03       object\n",
      "Var07       int64\n",
      "Var10       float64\n",
      "Var11       object\n",
      "Var13       object\n",
      "Var14       object\n",
      "Var19       object\n",
      "Var20       object\n",
      "Var21       object\n",
      "Var23       object\n",
      "Var25       object\n",
      "Var26       float64\n",
      "Var27       float64\n",
      "Var28       float64\n",
      "Var29       float64\n",
      "Var30       int64\n",
      "Var31       int64\n",
      "Var32       int64\n",
      "Var33       int64\n",
      "Var34       int64\n",
      "Var35       int64\n",
      "Var42       int64\n",
      "pred_s      float64\n",
      "Var47       float64\n",
      "Var48       object\n",
      "Var49       object\n",
      "Var50       int64\n",
      "Var51       object\n",
      "Var52       object\n",
      "Var53       object\n",
      "Var55       object\n",
      "c01         object\n",
      "c02         object\n",
      "c03         object\n",
      "c06         object\n",
      "c07         object\n",
      "c08         float64\n",
      "c09         float64\n",
      "c10         float64\n",
      "c11         float64\n",
      "c12         float64\n",
      "c13         object\n",
      "c14         object\n",
      "c15         float64\n",
      "c17         object\n",
      "c18         float64\n",
      "c19         float64\n",
      "c20         object\n",
      "c21         object\n",
      "c22         object\n",
      "c23         object\n",
      "c24         object\n",
      "c25         object\n",
      "c27         float64\n",
      "c28         object\n",
      "c29         object\n",
      "c30         object\n",
      "c31         object\n",
      "c32         object\n",
      "c33         object\n",
      "c36         object\n",
      "c37         object\n",
      "Var54       int64\n",
      "Var56       object\n",
      "Var02       float64\n",
      "Var04       int64\n",
      "Var05       int64\n",
      "Var06       int64\n",
      "Var08       int64\n",
      "Var09       int64\n",
      "Var12       float64\n",
      "Var15       int64\n",
      "Var16       int64\n",
      "Var18       int64\n",
      "Var22       int64\n",
      "Var24       int64\n",
      "c04         float64\n",
      "c05         float64\n",
      "c16         float64\n",
      "c26         float64\n",
      "c34         float64\n",
      "c35         float64\n",
      "c38         float64\n",
      "c39         float64\n",
      "c40         float64\n",
      "id          float64\n",
      "train       object\n",
      "dtypes: float64(28), int64(21), object(40)\n",
      "memory usage: 1.2+ GB\n"
     ]
    }
   ],
   "source": [
    "df_all.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ioBOhY2TQJPn",
    "outputId": "94849144-7ef5-4066-92f9-57e4e64f554e"
   },
   "outputs": [],
   "source": [
    "df_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bnsE_H19cCPt"
   },
   "source": [
    "# 02a_DataManipulation_basic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XDHLX2C8QJPt"
   },
   "source": [
    "#### Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SINLrVuAQJPt",
    "outputId": "d21ada1c-6f68-46d7-b7fc-a26391437332"
   },
   "outputs": [],
   "source": [
    "## check duplicates\n",
    "duplicateRowsDF = df_all[df_all.duplicated()]\n",
    "print(\"Duplicate Rows except first occurrence based on all columns are :\")\n",
    "duplicateRowsDF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bU5SurAHQJPw"
   },
   "outputs": [],
   "source": [
    "df_all = df_all.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "myB0c3EAQJPx",
    "outputId": "d2ebf90a-92f5-4057-813d-8f03149484ec"
   },
   "outputs": [],
   "source": [
    "print(df_all.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9G3APZzldINX"
   },
   "source": [
    "# 02b_DataManipulations_missings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### replacing missing by the mode if it is categorical and the mean if numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "plQoQuQjQJP-"
   },
   "outputs": [],
   "source": [
    "## check missing values\n",
    "def check_missings(df):\n",
    "    total = df.isnull().sum()\n",
    "    percent = df.isnull().sum()/df.isnull().count()*100\n",
    "    missing = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "    return missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WJa6BkeUQJQA"
   },
   "outputs": [],
   "source": [
    "missing_list = check_missings(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Total    Percent\n",
      "c04     728781  40.620187\n",
      "c39     444988  24.802370\n",
      "c40     439965  24.522402\n",
      "c38     427914  23.850714\n",
      "c16     260952  14.544725\n",
      "c05     253812  14.146762\n",
      "id      192542  10.731745\n",
      "c18     172688   9.625140\n",
      "c15     170015   9.476154\n",
      "c08     169206   9.431063\n",
      "c09     169206   9.431063\n",
      "c10     169206   9.431063\n",
      "pred_s  147947   8.246146\n",
      "Var47   147865   8.241576\n",
      "c19      78619   4.382000\n",
      "c26      60661   3.381072\n",
      "c27      42452   2.366154\n",
      "c34      28487   1.587785\n",
      "c35      28487   1.587785\n",
      "c12      12204   0.680216\n",
      "c11      10673   0.594883\n",
      "Var12     6587   0.367141\n",
      "Var02       13   0.000725\n",
      "Var10       13   0.000725\n",
      "Var29        5   0.000279\n",
      "Var28        5   0.000279\n",
      "Var26        4   0.000223\n",
      "Var27        4   0.000223\n"
     ]
    }
   ],
   "source": [
    "print(missing_list[missing_list.Total>0].sort_values(by='Total',ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicator_cols = missing_list.loc[missing_list['Total']>0].drop_duplicates().index.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Var10',\n",
       " 'Var26',\n",
       " 'Var28',\n",
       " 'pred_s',\n",
       " 'Var47',\n",
       " 'c08',\n",
       " 'c11',\n",
       " 'c12',\n",
       " 'c15',\n",
       " 'c18',\n",
       " 'c19',\n",
       " 'c27',\n",
       " 'Var12',\n",
       " 'c04',\n",
       " 'c05',\n",
       " 'c16',\n",
       " 'c26',\n",
       " 'c34',\n",
       " 'c38',\n",
       " 'c39',\n",
       " 'c40',\n",
       " 'id']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indicator_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>** A little confused with the following instruction. **<br> So you could create one missing indicator variable and use it for all of them.  Then for example if you create a logistic model and the target follows a different  pattern for examples with imputed mean than for cases that really are at the mean  (which is almost certainly will) then the missing indicator can pick that up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(df_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = ['Var47']\n",
    "\n",
    "labels = ['uniqueID','id', 'train', 'Var47']\n",
    "\n",
    "features = ['Var01', 'Var03', 'Var07', 'Var10', 'Var11', 'Var13',\n",
    "             'Var14', 'Var19', 'Var20', 'Var21', 'Var23', 'Var25', 'Var26',\n",
    "             'Var27', 'Var28', 'Var29', 'Var30', 'Var31', 'Var32', 'Var33', \n",
    "             'Var34', 'Var35', 'Var42', 'Var48', 'Var49', \n",
    "             'Var50', 'Var51', 'Var52', 'Var53', 'Var55', \n",
    "             'c01', 'c02', 'c03', 'c06', 'c07', 'c08', 'c09', 'c10', 'c11', 'c12', \n",
    "             'c13', 'c14', 'c15', 'c17', 'c18', 'c19', 'c20', 'c21', 'c22', 'c23', \n",
    "             'c24', 'c25', 'c27', 'c28', 'c29', 'c30', 'c31', 'c32', 'c33', 'c36', 'c37', \n",
    "             'Var54', 'Var56', 'Var02', 'Var04', 'Var05', 'Var06', 'Var08', 'Var09', \n",
    "             'Var12', 'Var15', 'Var16', 'Var18', 'Var22', 'Var24', \n",
    "             'c04', 'c05', 'c16', 'c26', 'c34', 'c35', 'c38', 'c39', 'c40']\n",
    "\n",
    "\n",
    "na_indicator_cols = ['Var10', 'Var26', 'Var28', 'Var47', 'c08','c11', 'c12', 'c15',\n",
    "                          'c18', 'c19', 'c27', 'Var12', 'c04', 'c05', 'c16', 'c26', \n",
    "                          'c34', 'c38', 'c39', 'c40',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_na_indicators = df_all[na_indicator_cols].isnull().astype(int).add_suffix('_NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove duplicated Missing Indicators\n",
    "\n",
    "# df_na_indicators = df_all[features].isnull().astype(int).add_suffix('_NA')\n",
    "\n",
    "# def duplicate_columns(df):\n",
    "#     \"\"\"Find duplicate columns in a DataFrame based on their values.\n",
    "    \n",
    "#     [Source] https://github.com/pandas-dev/pandas/issues/11250 \n",
    "    \n",
    "#     \"\"\"   \n",
    "#     groups = df.columns.to_series().groupby(df.dtypes).groups\n",
    "#     dups = []\n",
    "\n",
    "#     for t, v in groups.items():\n",
    "\n",
    "#         cs = df[v].columns\n",
    "#         vs = df[v]\n",
    "#         lcs = len(cs)\n",
    "\n",
    "#         for i in range(lcs):\n",
    "#             iv = vs.iloc[:,i].tolist()\n",
    "#             for j in range(i+1, lcs):\n",
    "#                 jv = vs.iloc[:,j].tolist()\n",
    "#                 if iv == jv:\n",
    "#                     dups.append(cs[i])\n",
    "#                     break\n",
    "\n",
    "#     return dups\n",
    "\n",
    "\n",
    "# dup_col_list = duplicate_columns(df_na_indicators)\n",
    "# df_na_indicators = df_na_indicators.drop(duplicate_columns_list, axis=1)\n",
    "# print(df_na_indicators.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SxfBMyrXdH8H"
   },
   "outputs": [],
   "source": [
    "## Impute missing values \n",
    "## [Source] https://stackoverflow.com/questions/25239958/impute-categorical-missing-values-in-scikit-learn\n",
    "\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "class DataFrameImputer(TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Impute missing values.\n",
    "\n",
    "        Columns of dtype object are imputed with the most frequent value \n",
    "        in column.\n",
    "\n",
    "        Columns of other types are imputed with mean of column.\n",
    "\n",
    "        \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "\n",
    "        self.fill = pd.Series([X[c].value_counts().index[0]\n",
    "            if X[c].dtype == np.dtype('O') else X[c].mean() for c in X],index=X.columns)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return X.fillna(self.fill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_filled = DataFrameImputer().fit_transform(df_all)\n",
    "# print(df_all_filled.head().transpose())\n",
    "\n",
    "df_all_a = pd.concat([df_all_filled, df_na_indicators], axis=1) \n",
    "\n",
    "df_all_b = pd.get_dummies(df_all_a)\n",
    "\n",
    "# with open('../final_project_data/insurance_clf_data_new.pickle','wb') as file:\n",
    "#     pickle.dump(df_all_b, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Za5cbbpUQJP0"
   },
   "source": [
    "# 03a_EDA_basic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pYs365bjQJP0"
   },
   "source": [
    "### check for distributions, correlation, ...\n",
    "<p>Q. how to deal with categorical data?\n",
    "<p>Todo. correlcation analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "36VD1i8CQJP1"
   },
   "source": [
    "#### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ACsjnO3mQJP1",
    "outputId": "2acdb071-83d8-4507-db1e-6f0f46257185"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1646270, 89)\n"
     ]
    }
   ],
   "source": [
    "## training set\n",
    "df_train = df_all.loc[df_all.train==\"Y\"]\n",
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WCNYXawrQJP3",
    "outputId": "49a9c98e-c789-4bc2-9a31-06f839ec8c7b"
   },
   "outputs": [],
   "source": [
    "## test set\n",
    "df_test = df_all.loc[df_all.train==\"N\"]\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CYBS9CuRQJP7"
   },
   "outputs": [],
   "source": [
    "# # Split numeric and categorical features for training set\n",
    "# cat_train_feats = data_train.select_dtypes(include=['object']).columns\n",
    "# num_train_feats = data_train.select_dtypes(include=['float64','int64']).columns\n",
    "# num_train_df = data_train[num_train_feats]\n",
    "# cat_train_df = data_train[cat_train_feats]\n",
    "# print('There are %d numeric features and %d categorical features in the training dataset\\n' %(len(num_train_feats),len(cat_train_feats)))\n",
    "# print('Numeric features:\\n',num_train_feats.values)\n",
    "# print('Categorical features:\\n',cat_train_feats.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bDCHKsCbQJQZ"
   },
   "source": [
    "#### Unique values within each variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zW0IOjA7QJQa",
    "outputId": "e6b47fb2-4e88-4647-e853-83c0d6aaea40",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Look at some of the unique values within each variable | traning set\n",
    "for col in list(data_train.columns):\n",
    "    uniques = data_train[col].unique() # get a list of unique values  \n",
    "    uniques.sort()\n",
    "    # if number of unique values is less than 30, print the values. Otherwise print the number of unique values\n",
    "    if len(uniques)<30:\n",
    "        print(col + ':')\n",
    "        print(uniques)\n",
    "    else:\n",
    "        print(col + ': ' + str(len(uniques)) + ' unique values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1_B5saRRQJQG"
   },
   "source": [
    "# 03a_EDA_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p87MypcMQJQG"
   },
   "outputs": [],
   "source": [
    "def cat_barplot(var):\n",
    "    clean_var = df_train[var].fillna('Missing')\n",
    "    var_counts = clean_var.value_counts()\n",
    "    barplot = sns.barplot(x=var_counts.index, y=var_counts.values, alpha=1)\n",
    "    return var_counts, barplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qeBzmHNfQJQI",
    "outputId": "fa443b15-60a4-4dfa-bc90-eab54b72d258"
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(16,16))\n",
    "# plt.subplot(221); cat_barplot('Var03')\n",
    "# plt.subplot(222); cat_barplot('Var01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qeBzmHNfQJQI",
    "outputId": "fa443b15-60a4-4dfa-bc90-eab54b72d258"
   },
   "outputs": [],
   "source": [
    "cat_barplot('c04')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qeBzmHNfQJQI",
    "outputId": "fa443b15-60a4-4dfa-bc90-eab54b72d258"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,16))\n",
    "cat_barplot('c05')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03b_EDA_boxplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mMW9QQhIQJQP"
   },
   "source": [
    "#### Categorical data & odrinary data, relation with classification target 0/1 (Boxplots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "uajHEA8HQJQP",
    "outputId": "6006ff46-a647-40e9-d874-93bc9eef14f9"
   },
   "outputs": [],
   "source": [
    "df_train.boxplot('c04','Var47', rot = 30,figsize=(5,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03a_EDA_pred_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4KheMpTbQJQR"
   },
   "source": [
    "#### Exsiting linear model accuracy pred_s  |  threshold = 50\n",
    "<p> the following could be replaced by confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IH5sZhvEQJQS",
    "outputId": "a0d337af-d85f-46a7-a7fc-aabb7850f442"
   },
   "outputs": [],
   "source": [
    "Var47_counts = data_train['Var47'].value_counts()\n",
    "print(Var47_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0fYpS2zgQJQV",
    "outputId": "d6e1ca53-f57e-491b-d4ea-09131f376415"
   },
   "outputs": [],
   "source": [
    "TP = len(data_train[(data_train['pred_s']>=50) & (data_train['Var47']==1)]) \n",
    "FN = len(data_train[(data_train['pred_s']<50) & (data_train['Var47']==0)]) \n",
    "accuracy = (TP+FN)/(data_train.shape[0])\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XokWyc5lQJQY"
   },
   "source": [
    "# 04a_LinearModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uhZDzXybQJQZ"
   },
   "source": [
    "<p>Q. How to impute missing values\n",
    "<p>Q. What could we do with categorical data ABCDE? What could we do with discreate numerical data 1 2 3 4 5?\n",
    "<p>Q. In theory, we can only use training set for model building, however, the categorical data make onehot encoding complicated if we conduct it for train and test set seperately. And error \"X has 254 features per sample; expecting 273\" happened. Is there any good way to do it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read the data\n",
    "with open('../final_project_data/insurance_clf_data_new.pickle','rb') as file:\n",
    "    df_all = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1794135 entries, 0 to 1795671\n",
      "Columns: 299 entries, uniqueID to train_Y\n",
      "dtypes: float64(28), int64(41), uint8(230)\n",
      "memory usage: 1.3 GB\n"
     ]
    }
   ],
   "source": [
    "df_all.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CQlM9pS_QJQq"
   },
   "source": [
    "#### Select input data for training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EMLlok2sQJQq",
    "outputId": "59a56cc6-78fe-4006-e55b-1e3a7165cfea"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>uniqueID</td>\n",
       "      <td>Var07</td>\n",
       "      <td>Var10</td>\n",
       "      <td>Var26</td>\n",
       "      <td>Var27</td>\n",
       "      <td>Var28</td>\n",
       "      <td>Var29</td>\n",
       "      <td>Var30</td>\n",
       "      <td>Var31</td>\n",
       "      <td>Var32</td>\n",
       "      <td>Var33</td>\n",
       "      <td>Var34</td>\n",
       "      <td>Var35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Var42</td>\n",
       "      <td>pred_s</td>\n",
       "      <td>Var47</td>\n",
       "      <td>Var50</td>\n",
       "      <td>c08</td>\n",
       "      <td>c09</td>\n",
       "      <td>c10</td>\n",
       "      <td>c11</td>\n",
       "      <td>c12</td>\n",
       "      <td>c15</td>\n",
       "      <td>c18</td>\n",
       "      <td>c19</td>\n",
       "      <td>c27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Var54</td>\n",
       "      <td>Var02</td>\n",
       "      <td>Var04</td>\n",
       "      <td>Var05</td>\n",
       "      <td>Var06</td>\n",
       "      <td>Var08</td>\n",
       "      <td>Var09</td>\n",
       "      <td>Var12</td>\n",
       "      <td>Var15</td>\n",
       "      <td>Var16</td>\n",
       "      <td>Var18</td>\n",
       "      <td>Var22</td>\n",
       "      <td>Var24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c04</td>\n",
       "      <td>c05</td>\n",
       "      <td>c16</td>\n",
       "      <td>c26</td>\n",
       "      <td>c34</td>\n",
       "      <td>c35</td>\n",
       "      <td>c38</td>\n",
       "      <td>c39</td>\n",
       "      <td>c40</td>\n",
       "      <td>id</td>\n",
       "      <td>Var10_NA</td>\n",
       "      <td>Var26_NA</td>\n",
       "      <td>Var28_NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Var47_NA</td>\n",
       "      <td>c08_NA</td>\n",
       "      <td>c11_NA</td>\n",
       "      <td>c12_NA</td>\n",
       "      <td>c15_NA</td>\n",
       "      <td>c18_NA</td>\n",
       "      <td>c19_NA</td>\n",
       "      <td>c27_NA</td>\n",
       "      <td>Var12_NA</td>\n",
       "      <td>c04_NA</td>\n",
       "      <td>c05_NA</td>\n",
       "      <td>c16_NA</td>\n",
       "      <td>c26_NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>c34_NA</td>\n",
       "      <td>c38_NA</td>\n",
       "      <td>c39_NA</td>\n",
       "      <td>c40_NA</td>\n",
       "      <td>Var01_A</td>\n",
       "      <td>Var01_B</td>\n",
       "      <td>Var01_C</td>\n",
       "      <td>Var03_A</td>\n",
       "      <td>Var03_B</td>\n",
       "      <td>Var03_C</td>\n",
       "      <td>Var03_D</td>\n",
       "      <td>Var03_E</td>\n",
       "      <td>Var03_F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Var03_G</td>\n",
       "      <td>Var03_H</td>\n",
       "      <td>Var03_I</td>\n",
       "      <td>Var03_J</td>\n",
       "      <td>Var03_L</td>\n",
       "      <td>Var03_N</td>\n",
       "      <td>Var03_P</td>\n",
       "      <td>Var03_Q</td>\n",
       "      <td>Var03_Z</td>\n",
       "      <td>Var11_A</td>\n",
       "      <td>Var11_B</td>\n",
       "      <td>Var11_C</td>\n",
       "      <td>Var11_D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Var11_E</td>\n",
       "      <td>Var11_F</td>\n",
       "      <td>Var11_G</td>\n",
       "      <td>Var11_Z</td>\n",
       "      <td>Var13_A</td>\n",
       "      <td>Var13_B</td>\n",
       "      <td>Var13_C</td>\n",
       "      <td>Var13_D</td>\n",
       "      <td>Var13_E</td>\n",
       "      <td>Var13_F</td>\n",
       "      <td>Var13_G</td>\n",
       "      <td>Var13_H</td>\n",
       "      <td>Var13_I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Var13_J</td>\n",
       "      <td>Var13_K</td>\n",
       "      <td>Var13_L</td>\n",
       "      <td>Var13_M</td>\n",
       "      <td>Var13_N</td>\n",
       "      <td>Var13_O</td>\n",
       "      <td>Var13_P</td>\n",
       "      <td>Var13_Q</td>\n",
       "      <td>Var13_R</td>\n",
       "      <td>Var13_S</td>\n",
       "      <td>Var13_Z</td>\n",
       "      <td>Var14_A</td>\n",
       "      <td>Var14_B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Var14_C</td>\n",
       "      <td>Var14_D</td>\n",
       "      <td>Var14_E</td>\n",
       "      <td>Var14_F</td>\n",
       "      <td>Var14_Z</td>\n",
       "      <td>Var19_A</td>\n",
       "      <td>Var19_B</td>\n",
       "      <td>Var19_C</td>\n",
       "      <td>Var19_D</td>\n",
       "      <td>Var19_E</td>\n",
       "      <td>Var19_F</td>\n",
       "      <td>Var19_G</td>\n",
       "      <td>Var19_Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Var20_A</td>\n",
       "      <td>Var20_B</td>\n",
       "      <td>Var21_A</td>\n",
       "      <td>Var21_B</td>\n",
       "      <td>Var21_C</td>\n",
       "      <td>Var21_D</td>\n",
       "      <td>Var21_Z</td>\n",
       "      <td>Var23_A</td>\n",
       "      <td>Var23_B</td>\n",
       "      <td>Var23_C</td>\n",
       "      <td>Var23_Z</td>\n",
       "      <td>Var25_A</td>\n",
       "      <td>Var25_B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Var25_C</td>\n",
       "      <td>Var25_D</td>\n",
       "      <td>Var25_E</td>\n",
       "      <td>Var25_F</td>\n",
       "      <td>Var25_G</td>\n",
       "      <td>Var25_H</td>\n",
       "      <td>Var25_I</td>\n",
       "      <td>Var25_J</td>\n",
       "      <td>Var25_K</td>\n",
       "      <td>Var25_L</td>\n",
       "      <td>Var25_M</td>\n",
       "      <td>Var25_Z</td>\n",
       "      <td>Var48_A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Var48_B</td>\n",
       "      <td>Var48_C</td>\n",
       "      <td>Var48_D</td>\n",
       "      <td>Var48_E</td>\n",
       "      <td>Var48_F</td>\n",
       "      <td>Var48_G</td>\n",
       "      <td>Var48_H</td>\n",
       "      <td>Var48_I</td>\n",
       "      <td>Var48_J</td>\n",
       "      <td>Var48_K</td>\n",
       "      <td>Var48_L</td>\n",
       "      <td>Var48_M</td>\n",
       "      <td>Var48_N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Var48_O</td>\n",
       "      <td>Var48_P</td>\n",
       "      <td>Var48_Q</td>\n",
       "      <td>Var49_A</td>\n",
       "      <td>Var49_B</td>\n",
       "      <td>Var51_A</td>\n",
       "      <td>Var51_B</td>\n",
       "      <td>Var51_C</td>\n",
       "      <td>Var51_D</td>\n",
       "      <td>Var51_E</td>\n",
       "      <td>Var51_F</td>\n",
       "      <td>Var51_G</td>\n",
       "      <td>Var51_H</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Var51_I</td>\n",
       "      <td>Var51_J</td>\n",
       "      <td>Var51_L</td>\n",
       "      <td>Var51_M</td>\n",
       "      <td>Var51_N</td>\n",
       "      <td>Var51_P</td>\n",
       "      <td>Var51_Q</td>\n",
       "      <td>Var51_Z</td>\n",
       "      <td>Var52_A</td>\n",
       "      <td>Var52_B</td>\n",
       "      <td>Var53_A</td>\n",
       "      <td>Var53_B</td>\n",
       "      <td>Var53_C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Var53_D</td>\n",
       "      <td>Var53_E</td>\n",
       "      <td>Var53_F</td>\n",
       "      <td>Var53_X</td>\n",
       "      <td>Var55_A</td>\n",
       "      <td>Var55_B</td>\n",
       "      <td>Var55_C</td>\n",
       "      <td>Var55_D</td>\n",
       "      <td>Var55_E</td>\n",
       "      <td>Var55_F</td>\n",
       "      <td>Var55_G</td>\n",
       "      <td>Var55_X</td>\n",
       "      <td>c01_A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>c01_B</td>\n",
       "      <td>c01_C</td>\n",
       "      <td>c01_D</td>\n",
       "      <td>c01_F</td>\n",
       "      <td>c01_G</td>\n",
       "      <td>c01_H</td>\n",
       "      <td>c02_A</td>\n",
       "      <td>c02_B</td>\n",
       "      <td>c02_C</td>\n",
       "      <td>c02_D</td>\n",
       "      <td>c02_E</td>\n",
       "      <td>c02_F</td>\n",
       "      <td>c02_G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>c02_H</td>\n",
       "      <td>c03_A</td>\n",
       "      <td>c03_B</td>\n",
       "      <td>c03_C</td>\n",
       "      <td>c03_D</td>\n",
       "      <td>c03_E</td>\n",
       "      <td>c03_F</td>\n",
       "      <td>c03_G</td>\n",
       "      <td>c03_H</td>\n",
       "      <td>c03_I</td>\n",
       "      <td>c03_J</td>\n",
       "      <td>c03_K</td>\n",
       "      <td>c03_L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>c06_N</td>\n",
       "      <td>c06_Y</td>\n",
       "      <td>c07_N</td>\n",
       "      <td>c07_Y</td>\n",
       "      <td>c13_A</td>\n",
       "      <td>c13_B</td>\n",
       "      <td>c13_C</td>\n",
       "      <td>c13_D</td>\n",
       "      <td>c13_E</td>\n",
       "      <td>c13_F</td>\n",
       "      <td>c13_G</td>\n",
       "      <td>c13_H</td>\n",
       "      <td>c13_I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>c14_N</td>\n",
       "      <td>c14_O</td>\n",
       "      <td>c14_Y</td>\n",
       "      <td>c17_A</td>\n",
       "      <td>c17_B</td>\n",
       "      <td>c17_C</td>\n",
       "      <td>c17_E</td>\n",
       "      <td>c17_F</td>\n",
       "      <td>c20_N</td>\n",
       "      <td>c20_O</td>\n",
       "      <td>c20_Y</td>\n",
       "      <td>c21_N</td>\n",
       "      <td>c21_O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>c21_Y</td>\n",
       "      <td>c22_E</td>\n",
       "      <td>c22_F</td>\n",
       "      <td>c23_N</td>\n",
       "      <td>c23_Y</td>\n",
       "      <td>c24_N</td>\n",
       "      <td>c24_Y</td>\n",
       "      <td>c25_A</td>\n",
       "      <td>c25_B</td>\n",
       "      <td>c25_C</td>\n",
       "      <td>c25_D</td>\n",
       "      <td>c25_E</td>\n",
       "      <td>c25_F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>c28_A</td>\n",
       "      <td>c28_B</td>\n",
       "      <td>c28_C</td>\n",
       "      <td>c29_A</td>\n",
       "      <td>c29_B</td>\n",
       "      <td>c29_C</td>\n",
       "      <td>c30_A</td>\n",
       "      <td>c30_B</td>\n",
       "      <td>c30_C</td>\n",
       "      <td>c31_A</td>\n",
       "      <td>c31_B</td>\n",
       "      <td>c31_C</td>\n",
       "      <td>c32_A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>c32_B</td>\n",
       "      <td>c32_C</td>\n",
       "      <td>c33_A</td>\n",
       "      <td>c33_B</td>\n",
       "      <td>c33_C</td>\n",
       "      <td>c36_N</td>\n",
       "      <td>c36_Y</td>\n",
       "      <td>c37_N</td>\n",
       "      <td>c37_Y</td>\n",
       "      <td>Var56_A</td>\n",
       "      <td>Var56_B</td>\n",
       "      <td>train_N</td>\n",
       "      <td>train_Y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0        1        2        3        4        5        6        7   \\\n",
       "0   uniqueID    Var07    Var10    Var26    Var27    Var28    Var29    Var30   \n",
       "1      Var42   pred_s    Var47    Var50      c08      c09      c10      c11   \n",
       "2      Var54    Var02    Var04    Var05    Var06    Var08    Var09    Var12   \n",
       "3        c04      c05      c16      c26      c34      c35      c38      c39   \n",
       "4   Var47_NA   c08_NA   c11_NA   c12_NA   c15_NA   c18_NA   c19_NA   c27_NA   \n",
       "5     c34_NA   c38_NA   c39_NA   c40_NA  Var01_A  Var01_B  Var01_C  Var03_A   \n",
       "6    Var03_G  Var03_H  Var03_I  Var03_J  Var03_L  Var03_N  Var03_P  Var03_Q   \n",
       "7    Var11_E  Var11_F  Var11_G  Var11_Z  Var13_A  Var13_B  Var13_C  Var13_D   \n",
       "8    Var13_J  Var13_K  Var13_L  Var13_M  Var13_N  Var13_O  Var13_P  Var13_Q   \n",
       "9    Var14_C  Var14_D  Var14_E  Var14_F  Var14_Z  Var19_A  Var19_B  Var19_C   \n",
       "10   Var20_A  Var20_B  Var21_A  Var21_B  Var21_C  Var21_D  Var21_Z  Var23_A   \n",
       "11   Var25_C  Var25_D  Var25_E  Var25_F  Var25_G  Var25_H  Var25_I  Var25_J   \n",
       "12   Var48_B  Var48_C  Var48_D  Var48_E  Var48_F  Var48_G  Var48_H  Var48_I   \n",
       "13   Var48_O  Var48_P  Var48_Q  Var49_A  Var49_B  Var51_A  Var51_B  Var51_C   \n",
       "14   Var51_I  Var51_J  Var51_L  Var51_M  Var51_N  Var51_P  Var51_Q  Var51_Z   \n",
       "15   Var53_D  Var53_E  Var53_F  Var53_X  Var55_A  Var55_B  Var55_C  Var55_D   \n",
       "16     c01_B    c01_C    c01_D    c01_F    c01_G    c01_H    c02_A    c02_B   \n",
       "17     c02_H    c03_A    c03_B    c03_C    c03_D    c03_E    c03_F    c03_G   \n",
       "18     c06_N    c06_Y    c07_N    c07_Y    c13_A    c13_B    c13_C    c13_D   \n",
       "19     c14_N    c14_O    c14_Y    c17_A    c17_B    c17_C    c17_E    c17_F   \n",
       "20     c21_Y    c22_E    c22_F    c23_N    c23_Y    c24_N    c24_Y    c25_A   \n",
       "21     c28_A    c28_B    c28_C    c29_A    c29_B    c29_C    c30_A    c30_B   \n",
       "22     c32_B    c32_C    c33_A    c33_B    c33_C    c36_N    c36_Y    c37_N   \n",
       "\n",
       "          8        9         10        11        12  \n",
       "0      Var31    Var32     Var33     Var34     Var35  \n",
       "1        c12      c15       c18       c19       c27  \n",
       "2      Var15    Var16     Var18     Var22     Var24  \n",
       "3        c40       id  Var10_NA  Var26_NA  Var28_NA  \n",
       "4   Var12_NA   c04_NA    c05_NA    c16_NA    c26_NA  \n",
       "5    Var03_B  Var03_C   Var03_D   Var03_E   Var03_F  \n",
       "6    Var03_Z  Var11_A   Var11_B   Var11_C   Var11_D  \n",
       "7    Var13_E  Var13_F   Var13_G   Var13_H   Var13_I  \n",
       "8    Var13_R  Var13_S   Var13_Z   Var14_A   Var14_B  \n",
       "9    Var19_D  Var19_E   Var19_F   Var19_G   Var19_Z  \n",
       "10   Var23_B  Var23_C   Var23_Z   Var25_A   Var25_B  \n",
       "11   Var25_K  Var25_L   Var25_M   Var25_Z   Var48_A  \n",
       "12   Var48_J  Var48_K   Var48_L   Var48_M   Var48_N  \n",
       "13   Var51_D  Var51_E   Var51_F   Var51_G   Var51_H  \n",
       "14   Var52_A  Var52_B   Var53_A   Var53_B   Var53_C  \n",
       "15   Var55_E  Var55_F   Var55_G   Var55_X     c01_A  \n",
       "16     c02_C    c02_D     c02_E     c02_F     c02_G  \n",
       "17     c03_H    c03_I     c03_J     c03_K     c03_L  \n",
       "18     c13_E    c13_F     c13_G     c13_H     c13_I  \n",
       "19     c20_N    c20_O     c20_Y     c21_N     c21_O  \n",
       "20     c25_B    c25_C     c25_D     c25_E     c25_F  \n",
       "21     c30_C    c31_A     c31_B     c31_C     c32_A  \n",
       "22     c37_Y  Var56_A   Var56_B   train_N   train_Y  "
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## All variables in the transformed dataframe\n",
    "features = list(df_all)\n",
    "df_features = pd.DataFrame(np.array(list(df_all)).reshape(23, 13))\n",
    "df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JGQZ8mNsQJQs"
   },
   "outputs": [],
   "source": [
    "## setting the inputs the output for our classification task\n",
    "target = ['Var47']\n",
    "\n",
    "labels = ['uniqueID', 'id', 'train_N', 'train_Y', 'Var47','pred_s']\n",
    "\n",
    "variables = ['Var01', 'Var03', 'Var07', 'Var10', 'Var11', 'Var13',\n",
    "             'Var14', 'Var19', 'Var20', 'Var21', 'Var23', 'Var25', 'Var26',\n",
    "             'Var27', 'Var28', 'Var29', 'Var30', 'Var31', 'Var32', 'Var33', \n",
    "             'Var34', 'Var35', 'Var42', 'Var48', 'Var49', \n",
    "             'Var50', 'Var51', 'Var52', 'Var53', 'Var55', \n",
    "             'c01', 'c02', 'c03', 'c06', 'c07', 'c08', 'c09', 'c10', 'c11', 'c12', \n",
    "             'c13', 'c14', 'c15', 'c17', 'c18', 'c19', 'c20', 'c21', 'c22', 'c23', \n",
    "             'c24', 'c25', 'c27', 'c28', 'c29', 'c30', 'c31', 'c32', 'c33', 'c36', 'c37', \n",
    "             'Var54', 'Var56', 'Var02', 'Var04', 'Var05', 'Var06', 'Var08', 'Var09', \n",
    "             'Var12', 'Var15', 'Var16', 'Var18', 'Var22', 'Var24', \n",
    "             'c04', 'c05', 'c16', 'c26', 'c34', 'c35', 'c38', 'c39', 'c40']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "293\n"
     ]
    }
   ],
   "source": [
    "## setting the inputs the output for our classification task\n",
    "features_01 = list(set(features) - set(labels))\n",
    "print(len(features_01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### df_train & df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kMGtzUTIQJQ9",
    "outputId": "68554218-0384-4c70-d502-966974b37f5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1646270, 299) (147865, 299)\n"
     ]
    }
   ],
   "source": [
    "df_train = df_all.loc[df_all.train_Y==1]\n",
    "df_test = df_all.loc[df_all.train_Y==0]\n",
    "print(df_train.shape, df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EWD9zYaNQJQ8"
   },
   "source": [
    "#### X_train  & y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_SwnAYDWQJRA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1646270, 293) (1646270, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train_01 = df_train[features_01].values.astype(np.int)\n",
    "y_train = df_train[target].values.astype(np.int)\n",
    "print(X_train_01.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_SwnAYDWQJRA"
   },
   "source": [
    "#### X_test & y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hA9Xq-6nQJRB",
    "outputId": "8a26c496-aedd-4276-a753-bfbbf35395ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(147865, 293) (147865, 1)\n"
     ]
    }
   ],
   "source": [
    "X_test_01 = df_test[features_01].values.astype(np.int)\n",
    "y_test = df_test[target].values.astype(np.int)\n",
    "print(X_test_01.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TH_3ODdtQJRH"
   },
   "source": [
    "#### Sampling (This part needs furture improvement)\n",
    "<p> Limited by our computing resources, we are now using a small sample of data to train models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cw2niVHJQJRH",
    "outputId": "f7a343ae-1c14-4b4e-cdb1-793df40cd27f"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df_train_0_b, df_train_0_s = df_test_split(df_train_0, test_size=0.3, random_state=42)\n",
    "print(df_train_0_b.shape, df_train_0_s.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cAM62zvfQJRJ",
    "outputId": "5b2a8837-833b-4fc0-e9c0-1a1cf371cdad"
   },
   "outputs": [],
   "source": [
    "X0_train_s = df_train_0_s.drop(labels, axis=1).fillna(0).values.astype(np.int8)\n",
    "y0_train_s = df_train_0_s[target].values.astype(np.int8)\n",
    "print(X0_train_s.shape, y0_train_s.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cakrCWcPQJRL"
   },
   "source": [
    "<p> Considering stratified sampling?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z84PACZYQJRM"
   },
   "outputs": [],
   "source": [
    "# # Divide by 1.5 to limit the number of income categories\n",
    "# housing[\"income_cat\"] = np.ceil(housing[\"median_income\"] / 1.5)\n",
    "# # Label those above 5 as 5\n",
    "# housing[\"income_cat\"].where(housing[\"income_cat\"] < 5, 5.0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YRxLLctlQJRN"
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "# for train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n",
    "#     strat_train_set = housing.loc[train_index]\n",
    "#     strat_test_set = housing.loc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bsM3uciyQJRO"
   },
   "outputs": [],
   "source": [
    "# def income_cat_proportions(data):\n",
    "#     return data[\"income_cat\"].value_counts() / len(data)\n",
    "\n",
    "# train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n",
    "\n",
    "# compare_props = pd.DataFrame({\n",
    "#     \"Overall\": income_cat_proportions(housing),\n",
    "#     \"Stratified\": income_cat_proportions(strat_test_set),\n",
    "#     \"Random\": income_cat_proportions(test_set),\n",
    "# }).sort_index()\n",
    "# compare_props[\"Rand. %error\"] = 100 * compare_props[\"Random\"] / compare_props[\"Overall\"] - 100\n",
    "# compare_props[\"Strat. %error\"] = 100 * compare_props[\"Stratified\"] / compare_props[\"Overall\"] - 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qseiay1gQJRQ"
   },
   "outputs": [],
   "source": [
    "# compare_props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PFs7nnjjQJRU"
   },
   "outputs": [],
   "source": [
    "# for set_ in (strat_train_set, strat_test_set):\n",
    "#     set_.drop(\"income_cat\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OOLpCdgyQJRV"
   },
   "source": [
    "#### Performance measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4hGq9FRQQJRW"
   },
   "outputs": [],
   "source": [
    "# This is a function which outputs a variety of accuracy results. We can use these results to compare models.\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "def calc_specificity(y_vals, preds, threshes):\n",
    "    # calculates specificity\n",
    "    return sum((preds < threshes) & (y_vals == 0)) /sum(y_vals ==0)\n",
    "\n",
    "def print_report(y_vals, preds, threshes):\n",
    "    \n",
    "    auc = roc_auc_score(y_vals, preds)\n",
    "    accuracy = accuracy_score(y_vals, (preds > threshes))\n",
    "    recall = recall_score(y_vals, (preds > threshes))\n",
    "    precision = precision_score(y_vals, (preds > threshes))\n",
    "    specificity = calc_specificity(y_vals, preds, threshes)\n",
    "    print('AUC:%.3f'%auc)\n",
    "    print('accuracy:%.3f'%accuracy)\n",
    "    print('recall:%.3f'%recall)\n",
    "    print('precision:%.3f'%precision)\n",
    "    print('specificity:%.3f'%specificity)\n",
    "    print(' ')\n",
    "    return auc, accuracy, recall, precision, specificity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6qoIzYf9QJRW"
   },
   "source": [
    "#### Training models and validation\n",
    "<p> Toooo low validation accuracy now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zK_wKZ07QJRX"
   },
   "outputs": [],
   "source": [
    "# ## use a small sample; 30% of training data\n",
    "# X_train = X0_train_s\n",
    "# y_train = y0_train_s\n",
    "# X_test = X0_test\n",
    "# y_test = y0_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "X_train = X_train_01\n",
    "y_train = y_train\n",
    "X_test = X_test_01\n",
    "y_test = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O_N7ZAYgQJRY"
   },
   "outputs": [],
   "source": [
    "# prepare Logistic Regression classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logit_clf = LogisticRegression(tol=0.0001, C=1.0, random_state=42, max_iter=100,penalty='l1', solver='liblinear',verbose=0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KRO8f2P3QJRZ",
    "outputId": "afa8698c-2ca8-4e54-ddad-e5272664959f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 710.2725329399109 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# training and validation\n",
    "import time\n",
    "start_time = time.time()\n",
    "logit_clf.fit(X_train, y_train) \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o3pRyBaeQJRa"
   },
   "outputs": [],
   "source": [
    "y_train_logit_pred = logit_clf.predict(X_train)\n",
    "y_test_logit_pred = logit_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oiPmQqz5QJRb",
    "outputId": "a3c35c54-1b90-4ea9-d8d8-3e5e47336f75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy 0.7513706743122331\n",
      "Test Accuracy 0.5159300713488655\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Accuracy\", accuracy_score(y_train, y_train_logit_pred))\n",
    "print(\"Test Accuracy\", accuracy_score(y_test, y_test_logit_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Wts4PZoQJRe"
   },
   "source": [
    "# 05_a. DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MxfoGGQYQJRf"
   },
   "outputs": [],
   "source": [
    "## use all training data\n",
    "X_train = X0_train\n",
    "y_train = y0_train\n",
    "X_test = X0_test\n",
    "y_test = y0_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pOa_yVU1QJRg"
   },
   "outputs": [],
   "source": [
    "## prepare Random Forest classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_clf = RandomForestClassifier(random_state=42, max_depth = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HPNZulx5QJRi",
    "outputId": "7a7d047b-84ed-46f8-9710-b7edcac44a4d"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "rf_clf.fit(X_train, y_train) \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tnUkzU8pQJRk"
   },
   "outputs": [],
   "source": [
    "y_train_rf_pred = rf_clf.predict(X_train)\n",
    "y_test_rf_pred = rf_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YZCRKl1HQJRl",
    "outputId": "81cf165f-33d7-4b5a-bf86-059f4712bd08"
   },
   "outputs": [],
   "source": [
    "print(\"Training Accuracy\", accuracy_score(y_train, y_train_rf_pred))\n",
    "print(\"Test Accuracy\", accuracy_score(y_test, y_test_rf_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4FVdmqd4QJRo"
   },
   "source": [
    "#### Feature importance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "35RJ87IXQJRo"
   },
   "outputs": [],
   "source": [
    "## Source https://towardsdatascience.com/feature-selection-techniques-in-machine-learning-with-python-f24e7da3f36e\n",
    "X = X_train\n",
    "y = y_train\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(X,y)\n",
    "print(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n",
    "#plot graph of feature importances for better visualization\n",
    "feat_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "feat_importances.nlargest(10).plot(kind='barh')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7IfFSyEJQJRq"
   },
   "source": [
    "# 06_a.DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JP3k9Rc4QJRq"
   },
   "outputs": [],
   "source": [
    "X_train = X0_train_s\n",
    "y_train = y0_train_s\n",
    "X_test = X0_test\n",
    "y_test = y0_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lis7d894QJRr"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "# from tensorflow_graph_in_jupyter import show_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hIl9qwNwQJRs"
   },
   "outputs": [],
   "source": [
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NFakPR5cQJRt"
   },
   "outputs": [],
   "source": [
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \".\"    \n",
    "logdir = \"{}//run-{}//\".format(root_logdir, now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jLJHHdC0QJRu"
   },
   "outputs": [],
   "source": [
    "## define validation and training sets\n",
    "X_test = X_test.astype(np.int8)\n",
    "y_train = y_train.astype(np.int8)\n",
    "y_test = y_test.astype(np.int8)\n",
    "X_valid, X_train = X_train[:100], X_train[100:]\n",
    "y_valid, y_train = y_train[:100], y_train[100:]\n",
    "\n",
    "\n",
    "n_inputs = 273  # number of features \n",
    "n_hidden1 = 150   \n",
    "n_hidden2 = 70   \n",
    "# dedfine more hidden layers\n",
    "n_hidden3 = 50\n",
    "n_outputs = 2 \n",
    "tf.reset_default_graph()   # reset graph\n",
    "\n",
    "\n",
    "# y place holder for output\n",
    "# auxialrry variable \n",
    "X = tf.placeholder(tf.int8, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int8, shape=(None), name=\"y\")\n",
    "a = tf.placeholder(tf.int8, shape=(None),name='a')\n",
    "\n",
    "\n",
    "summary1 = tf.reduce_mean(a, name=\"summary1\")\n",
    "\n",
    "#layer of network takes input of neurones  \n",
    "def neuron_layer(X, n_neurons, name, activation=None):\n",
    "    with tf.name_scope(name):\n",
    "        n_inputs = int(X.get_shape()[1])\n",
    "        stddev = 2 / np.sqrt(n_inputs)\n",
    "        init = tf.truncated_normal((n_inputs, n_neurons), stddev=stddev)\n",
    "        W = tf.Variable(init, name=\"kernel\")\n",
    "        b = tf.Variable(tf.zeros([n_neurons]), name=\"bias\")\n",
    "        Z = tf.matmul(X, W) + b\n",
    "        if activation is not None:\n",
    "            return activation(Z)\n",
    "        else:\n",
    "            return Z\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = neuron_layer(X, n_hidden1, name=\"hidden1\", activation=tf.nn.relu)\n",
    "    hidden2 = neuron_layer(hidden1, n_hidden2, name=\"hidden2\", activation=tf.nn.relu)\n",
    "    hidden3 = neuron_layer(hidden2, n_hidden3, name=\"hidden3\", activation=tf.nn.relu)\n",
    "    logits  = neuron_layer(hidden3, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "#learning rate - change if issues regrading convergence or others happen\n",
    "learning_rate =1e-1\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdagradOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "#define accuracy \n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "#leave this\n",
    "def shuffle_batch(X, y, batch_size):\n",
    "    rnd_idx = np.random.permutation(len(X))\n",
    "    n_batches = len(X) // batch_size\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1kto9ZV7QJRv"
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "n_epochs = 100  \n",
    "batch_size = 100   \n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "summary2 = tf.summary.scalar('Accuracy', summary1)\n",
    "file_writer = tf.summary.FileWriter(logdir , tf.get_default_graph())\n",
    "\n",
    "start_time = time.time()\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    acc_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "    print(\"Val accuracy init:\", acc_val)\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_batch = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Batch accuracy:\", acc_batch, \"Val accuracy:\", acc_val)\n",
    "        \n",
    "        summary_str = summary2.eval(feed_dict={a: acc_val })\n",
    "        file_writer.add_summary(summary_str, epoch)\n",
    "   \n",
    "    saver.save(sess, \"./insurance_dnn_model.ckpt\")   \n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uxjey5S1QJRw"
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:                         \n",
    "    saver.restore(sess, \"./insurance_dnn_model.ckpt\")   \n",
    "    Z = logits.eval(feed_dict={X: X_test})\n",
    "    y_test_dnn_pred = np.argmax(Z, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZTro4em6QJRx"
   },
   "outputs": [],
   "source": [
    "print(\"Training Accuracy\", accuracy_score(y_train, y_train_dnn_pred))\n",
    "print(\"Test Accuracy\", accuracy_score(y_test, y_test_dnn_pred))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "OOLpCdgyQJRV",
    "4FVdmqd4QJRo"
   ],
   "name": "Insurance_classification_v5.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
